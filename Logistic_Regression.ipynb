{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ib-hcJCJ9PKP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Theoretical***"
      ],
      "metadata": {
        "id": "8SlEowEq92IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Q. 1 :- What is Logistic Regression, and how does it differ from Linear Regression.\n",
        "Ans :- Logistic Regression\n",
        "Despite the name, it’s actually used for classification, not regression. Its job is to estimate the probability that a data point belongs to a particular class (e.g. “Yes” or “No”, “Spam” or “Not Spam”). It does this using the logistic (sigmoid) function, which squashes the output of a linear combination of features into a range between 0 and 1. If the output probability is above a threshold (commonly 0.5), it assigns one class; otherwise, another.\n",
        "Linear Regression\n",
        "This one is for predicting continuous values—like forecasting sales, temperature, or house prices. It fits a straight line (or hyperplane) to the data by minimizing the mean squared error between predicted and actual values. The output can take any real number, positive or negative.\n",
        "\n",
        "Q. 2:- What is the mathematical equation of Logistic Regression.\n",
        "Ans :- Logistic Regression might wear a \"regression\" label, but its goal is to classify, not predict continuous outcomes. At the heart of it is a beautiful equation that wraps a linear function inside a sigmoid curve.\n",
        "Here’s the mathematical form:\n",
        "[ P(y = 1 \\mid \\mathbf{x}) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n)}} ]\n",
        "\n",
        "Q. 3:- Why do we use the Sigmoid function in Logistic Regression.\n",
        "Ans :- We use the sigmoid function in Logistic Regression because it transforms the output of a linear equation into a value between 0 and 1—just what we need when predicting probabilities for binary classification.\n",
        "Here’s what the sigmoid function looks like:\n",
        "[ \\sigma(z) = \\frac{1}{1 + e^{-z}} ]\n",
        "\n",
        "Q. 4 :- What is the cost function of Logistic Regression.\n",
        "Ans :- In logistic regression, the cost function is designed to measure how well the model's predicted probabilities match the actual class labels. Since logistic regression is used for binary classification, we can’t use the mean squared error (like in linear regression), because it would result in a non-convex function—making optimization tricky.\n",
        "Instead, we use the log loss (also called logistic loss or binary cross-entropy). Here's what it looks like:\n",
        "[ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right] ]\n",
        "Where:\n",
        "- ( m ) is the number of training examples\n",
        "- ( y^{(i)} ) is the actual label for example ( i )\n",
        "- ( h_\\theta(x^{(i)}) ) is the predicted probability that the label is 1 (using the sigmoid function)\n",
        "\n",
        "Q. 5 :- What is Regularization in Logistic Regression? Why is it needed.\n",
        "Ans :- Regularization in logistic regression is like a safeguard against overfitting. It adds a penalty to the cost function to discourage the model from becoming too complex or relying too heavily on any one feature.\n",
        "Why it's needed\n",
        "Without regularization, logistic regression might assign very large weights to certain features—especially in high-dimensional datasets—just to perfectly fit the training data. This can lead to a model that performs well on training data but poorly on unseen data (i.e., it doesn’t generalize well).\n",
        "\n",
        "Q. 6 :- Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "Ans :- Ridge Regression (L2 Regularization)\n",
        "Ridge adds a penalty equal to the square of the magnitude of coefficients. It shrinks coefficients but doesn’t eliminate them entirely. This is especially useful when you have many correlated features — it spreads the weight across them rather than picking just one.\n",
        "[ \\text{Loss} = \\text{RSS} + \\lambda \\sum_{j=1}2 ]\n",
        "- Keeps all features in the model\n",
        "- Good for multicollinearity\n",
        "- Doesn’t perform feature selection\n",
        "Lasso Regression (L1 Regularization)\n",
        "Lasso adds a penalty equal to the absolute value of the coefficients. This can shrink some coefficients to exactly zero, effectively performing feature selection.\n",
        "[ \\text{Loss} = \\text{RSS} + \\lambda \\sum_{j=1}^{n} |\\beta_j| ]\n",
        "- Can eliminate irrelevant features\n",
        "- Useful when you suspect only a few features are important\n",
        "- Can be unstable when features are highly correlated\n",
        "Elastic Net Regression\n",
        "Elastic Net is the best of both worlds — it combines L1 and L2 penalties. It’s especially helpful when you have many features and some are correlated.\n",
        "[ \\text{Loss} = \\text{RSS} + \\lambda \\left[ \\alpha \\sum |\\beta_j| + (1 - \\alpha) \\sum \\beta_j^2 \\right] ]\n",
        "- Balances between Ridge and Lasso\n",
        "- Performs feature selection and handles multicollinearity\n",
        "- Controlled by two parameters: ( \\lambda ) (penalty strength) and ( \\alpha ) (mixing ratio)\n",
        "\n",
        "Q. 7 :-  When should we use Elastic Net instead of Lasso or Ridge.\n",
        "Ans :- Elastic Net is your go-to when you’re dealing with many features, especially when some of them are highly correlated or when you suspect that only a few are truly important, but you're not sure which ones.\n",
        "Here’s when Elastic Net shines:\n",
        "- High-dimensional data: If the number of predictors is greater than the number of observations (like in genomics or text classification), Elastic Net helps by combining the strengths of both Lasso and Ridge.\n",
        "- Correlated features: Lasso tends to pick one feature from a group and ignore the rest, while Ridge spreads the weight. Elastic Net balances both, often selecting groups of correlated features together.\n",
        "- Model stability: Lasso can be unstable when predictors are highly correlated. Elastic Net smooths this out by blending in Ridge’s regularization\n",
        "\n",
        "Q. 8 :- What is the impact of the regularization parameter (λ) in Logistic Regression.\n",
        "Ans :- The regularization parameter ( \\lambda ) in logistic regression acts like a dial that controls the balance between fitting the training data well and keeping the model simple to avoid overfitting.\n",
        "Here's how it impacts the model:\n",
        "- Large ( \\lambda ): Strong regularization. This heavily penalizes large weights, pushing the model to keep coefficients small. It reduces variance but may increase bias — meaning the model might underfit the data.\n",
        "- Small ( \\lambda ): Weak regularization. The model has more freedom to fit the training data, which can lead to overfitting — especially if the dataset is noisy or high-dimensional.\n",
        "- ( \\lambda = 0 ): No regularization at all. The model behaves like standard logistic regression, which might perfectly fit the training data but generalize poorly.\n",
        "In practice, tuning ( \\lambda ) is crucial. It’s often done using cross-validation to find the sweet spot where the model performs best on unseen data.\n",
        "Fun fact: in libraries like scikit-learn, the parameter is often expressed as ( C = \\frac{1}{\\lambda} ), so a smaller ( C ) means stronger regularization.\n",
        "Want to try visualizing how different values of ( \\lambda ) affect decision boundaries or model accuracy? I can help set that up.\n",
        "\n",
        "\n",
        "Q. 9 :- What are the key assumptions of Logistic Regression.\n",
        "Ans :- Logistic regression may seem flexible, but it does rest on a few important assumptions to ensure its predictions are reliable. Here are the key ones:\n",
        "1. Binary or Categorical Outcome\n",
        "The dependent variable should be binary (e.g., 0 or 1, Yes or No). If there are more than two categories, you'd need multinomial or ordinal logistic regression instead.\n",
        "2. Independence of Observations\n",
        "Each observation should be independent of the others. This means no repeated measures or clustering unless you're using a model designed for that (like mixed-effects logistic regression).\n",
        "3. Linearity of Log Odds\n",
        "While logistic regression doesn’t assume a linear relationship between predictors and the outcome, it does assume a linear relationship between the predictors and the log odds of the outcome.\n",
        "\n",
        "4. No Multicollinearity\n",
        "Predictors should not be highly correlated with each other. Severe multicollinearity can distort the estimated coefficients and make interpretation difficult. Variance Inflation Factor (VIF) is often used to check this.\n",
        "5. No Extreme Outliers\n",
        "Outliers can have a big influence on the model’s coefficients. It’s good practice to detect and handle them before fitting the model—Cook’s distance is one way to do this.\n",
        "6. Large Sample Size\n",
        "Logistic regression performs best with a reasonably large dataset. Small sample sizes can lead to unstable estimates and poor generalization\n",
        "\n",
        "Q. 10 :- What are some alternatives to Logistic Regression for classification tasks.\n",
        "Ans :- There are plenty of powerful alternatives to logistic regression, each with its own strengths depending on the nature of your data and the problem you're tackling. Here’s a quick tour of the most popular ones:\n",
        "1. Decision Trees\n",
        "Simple and interpretable, decision trees split the data based on feature values. They’re great when you want a model that’s easy to explain, but they can overfit without pruning.\n",
        "2. Random Forests\n",
        "An ensemble of decision trees that reduces overfitting by averaging predictions. It’s robust, handles non-linear relationships well, and works great out of the box.\n",
        "3. Support Vector Machines (SVM)\n",
        "SVMs find the optimal hyperplane that separates classes. They’re especially effective in high-dimensional spaces and can handle non-linear boundaries using kernel tricks.\n",
        "4. K-Nearest Neighbors (KNN)\n",
        "A lazy learner that classifies based on the majority class among the k closest data points. It’s intuitive but can be slow with large datasets and sensitive to feature scaling.\n",
        "5. Naive Bayes\n",
        "Based on Bayes’ theorem with a strong independence assumption between features. Surprisingly effective for text classification and spam detection.\n",
        "6. Gradient Boosting Machines (GBM, XGBoost, LightGBM)\n",
        "These are ensemble methods that build models sequentially to correct errors from previous ones. They often outperform other models in structured data competitions.\n",
        "7. Neural Networks\n",
        "Especially useful when you have large datasets and complex patterns. They can model non-linear relationships but require more tuning and computational power.\n",
        "8. Poisson or Log-Binomial Regression\n",
        "These are statistical alternatives that can be used when logistic regression assumptions don’t hold, though they’re less common and can have convergence issues\n",
        "\n",
        "Q. 11 :- What are Classification Evaluation Metrics.\n",
        "Ans :- Classification evaluation metrics help us understand how well a model is performing at predicting class labels. Here’s a breakdown of the most commonly used ones:\n",
        "1. Accuracy\n",
        "The ratio of correctly predicted observations to the total observations. It’s intuitive but can be misleading if the classes are imbalanced.\n",
        "[ \\text{Accuracy} = \\frac{\\text{TP + TN}}{\\text{TP + TN + FP + FN}} ]\n",
        "2. Precision\n",
        "Out of all the positive predictions, how many were actually correct?\n",
        "[ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP + FP}} ]\n",
        "Useful when the cost of false positives is high (e.g., spam detection).\n",
        "3. Recall (Sensitivity or True Positive Rate)\n",
        "Out of all actual positives, how many did the model correctly identify?\n",
        "[ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP + FN}} ]\n",
        "Important when missing a positive case is costly (e.g., disease detection).\n",
        "4. F1 Score\n",
        "The harmonic mean of precision and recall. It balances the two, especially when you need a single metric to compare models.\n",
        "[ \\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision + Recall}} ]\n",
        "5. Confusion Matrix\n",
        "A table that shows the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). It gives a complete picture of model performance.\n",
        "6. AUC-ROC (Area Under the Receiver Operating Characteristic Curve)\n",
        "Measures the model’s ability to distinguish between classes. A higher AUC means better performance across different classification thresholds.\n",
        "7. Log Loss (Logarithmic Loss)\n",
        "Penalizes false classifications based on the confidence of the prediction. Lower log loss indicates better calibrated probabilities\n",
        "\n",
        "Q. 12 :- How does class imbalance affect Logistic Regression.\n",
        "Ans :- Class imbalance can throw a serious wrench into logistic regression’s performance, especially when one class significantly outweighs the other (like 95% vs 5%).\n",
        "Here's how it affects the model:\n",
        "- Biased predictions: Logistic regression tends to favor the majority class, often predicting it by default. This can lead to deceptively high accuracy but poor performance on the minority class — which might be the one you care about most (e.g., fraud detection, disease diagnosis).\n",
        "- Skewed decision boundary: The model may learn a decision boundary that’s too close to the minority class, making it harder to correctly classify those rare cases.\n",
        "- Poor recall for minority class: You might see high precision but very low recall for the minority class, meaning the model misses many true positives.\n",
        "\n",
        "Q. 13 :- What is Hyperparameter Tuning in Logistic Regression.\n",
        "Ans :- Hyperparameter tuning in logistic regression is the process of finding the best combination of settings that control how the model learns — without being directly learned from the data itself. These settings, or hyperparameters, influence the model’s complexity, generalization ability, and ultimately its performance.\n",
        "\n",
        "Q. 14 :- What are different solvers in Logistic Regression? Which one should be used.\n",
        "Ans :- In logistic regression, solvers are optimization algorithms used to minimize the cost function and find the best model parameters. Each solver has its own strengths depending on the dataset size, sparsity, and the type of regularization you’re using.\n",
        "\n",
        "Q. 15 :- How is Logistic Regression extended for multiclass classification.\n",
        "Ans :- Logistic regression is naturally built for binary classification, but it can be extended to handle multiclass classification using two main strategies:\n",
        "1. One-vs-Rest (OvR)\n",
        "Also called One-vs-All, this approach trains one binary classifier per class. Each classifier predicts whether a sample belongs to its class or not. During prediction, the model with the highest confidence wins.\n",
        "- Simple and widely supported (e.g., in scikit-learn)\n",
        "- Can be inefficient if the number of classes is large\n",
        "2. Multinomial Logistic Regression (Softmax Regression)\n",
        "This is a direct generalization of logistic regression for multiple classes. Instead of modeling the probability of one class, it models the probability distribution over all classes using the softmax function:\n",
        "[ P(y = k \\mid x) = \\frac{e^{\\theta_k{K} e^{\\theta_j^T x}} ]\n",
        "Where ( K ) is the number of classes, and each class has its own parameter vector ( \\theta_k ).\n",
        "- More elegant and efficient for multiclass problems\n",
        "- Requires solvers that support the 'multinomial' option (like lbfgs or saga in scikit-learn)\n",
        "When to Use What?\n",
        "- Use OvR if you're working with a simple model or a library that doesn’t support multinomial loss.\n",
        "- Use Multinomial Logistic Regression when you want a more probabilistically sound and efficient approach for multiclass tasks.\n",
        "If you’d like, I can show you how to implement both in Python or compare their performance on a dataset. Just say the word!.\n",
        "2. Multinomial Logistic Regression (Softmax Regression)\n",
        "This is a direct generalization of logistic regression for multiple classes. Instead of modeling the probability of one class, it models the probability distribution over all classes using the softmax function:\n",
        "[ P(y = k \\mid x) = \\frac{e^{\\theta_k{K} e^{\\theta_j^T x}} ]\n",
        "Where ( K ) is the number of classes, and each class has its own parameter vector ( \\theta_k ).\n",
        "- More elegant and efficient for multiclass problems\n",
        "- Requires solvers that support the 'multinomial' option (like lbfgs or saga in scikit-learn)\n",
        "\n",
        "Q. 16 :- What are the advantages and disadvantages of Logistic Regression.\n",
        "Ans :- Logistic regression is a classic for a reason — it’s simple, fast, and surprisingly effective in many situations. But like any tool, it has its trade-offs. Here's a balanced look:\n",
        "Advantages\n",
        "- Easy to implement and interpret: The math is straightforward, and the output (odds ratios, coefficients) is easy to explain — great for transparency.\n",
        "- Efficient to train: It’s computationally light, making it ideal for quick iterations or large datasets.\n",
        "- Probabilistic output: It doesn’t just give a class label — it gives the probability, which is useful for decision-making.\n",
        "- Works well with linearly separable data: If your classes are nicely divided by a straight line (or plane), logistic regression performs beautifully.\n",
        "- Extensible: It can be adapted for multiclass problems (via softmax) and regularized (L1, L2, Elastic Net) to improve generalization.\n",
        "Disadvantages\n",
        "- Assumes linearity in the log-odds: It can’t naturally capture complex, non-linear relationships unless you manually engineer features.\n",
        "- Sensitive to outliers: Extreme values can skew the model unless handled properly.\n",
        "- Struggles with multicollinearity: Highly correlated features can distort coefficient estimates.\n",
        "- Not ideal for small sample sizes: Especially when the number of features is large — it can overfit or produce unstable estimates.\n",
        "- Limited to binary or categorical outcomes: It’s not suitable for regression tasks or ordinal outcomes without modification.\n",
        "\n",
        "Q. 17 :- What are some use cases of Logistic Regression.\n",
        "Ans :- Logistic regression is a versatile workhorse in the machine learning world — especially when you need fast, interpretable models for classification. Here are some real-world use cases where it shines:\n",
        "1. Medical Diagnosis\n",
        "Predicting whether a patient has a disease (e.g., diabetes, cancer) based on clinical features like age, blood pressure, and test results. It’s favored for its interpretability in high-stakes decisions.\n",
        "2. Credit Scoring\n",
        "Banks use logistic regression to assess the likelihood of a borrower defaulting on a loan. It helps in making transparent, data-driven lending decisions.\n",
        "3. Marketing and Customer Retention\n",
        "Used to predict whether a customer will respond to a campaign or churn. Features might include purchase history, engagement metrics, or demographics.\n",
        "4. Spam Detection\n",
        "Classifies emails as spam or not spam based on word frequencies, sender info, and other metadata. It’s lightweight and effective for this binary task.\n",
        "5. Fraud Detection\n",
        "Helps flag potentially fraudulent transactions by modeling the probability of fraud based on transaction patterns, location, and user behavior.\n",
        "6. Voting Behavior Prediction\n",
        "Political analysts use it to predict whether someone will vote for a particular party based on survey responses and demographic data.\n",
        "7. Image Classification (Simplified Tasks)\n",
        "While not ideal for complex image tasks, logistic regression can be used for basic binary image classification — like distinguishing between handwritten digits 0 and 1.\n",
        "\n",
        "Q. 18 :- What is the difference between Softmax Regression and Logistic Regression.\n",
        "Ans :- Softmax regression and logistic regression are closely related — in fact, softmax is a generalization of logistic regression for multiclass problems.\n",
        "Logistic Regression\n",
        "- Used for binary classification (two classes: 0 or 1).\n",
        "- Uses the sigmoid function to map predictions to probabilities between 0 and 1.\n",
        "- Outputs a single probability for the positive class; the other is just 1 minus that.\n",
        "[ P(y = 1 \\mid x) = \\frac{1}{1 + e^{-\\theta^T x}} ]\n",
        "Softmax Regression (Multinomial Logistic Regression)\n",
        "- Used for multiclass classification (more than two classes).\n",
        "- Uses the softmax function to output a probability distribution over all classes.\n",
        "- Each class gets its own score, and the softmax normalizes them so they sum to 1.\n",
        "[ P(y = k \\mid x) = \\frac{e^{\\theta_k{K} e^{\\theta_j^T x}} ]\n",
        "\n",
        "Q. 19 :- How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification.\n",
        "Ans :- Choosing between One-vs-Rest (OvR) and Softmax (Multinomial Logistic Regression) depends on your dataset, model requirements, and computational constraints. Here's how to think about it:\n",
        " One-vs-Rest (OvR)\n",
        "- How it works: Trains one binary classifier per class. Each model learns to distinguish one class from all others.\n",
        "- When to use it:\n",
        "- You’re using a binary classifier (like SVM or basic logistic regression) that doesn’t natively support multiclass.\n",
        "- You want interpretability — each model is simpler and easier to analyze.\n",
        "- You have a small number of classes and limited computational resources.\n",
        "Softmax (Multinomial Logistic Regression)\n",
        "- How it works: Trains a single model that directly estimates the probability distribution across all classes using the softmax function.\n",
        "- When to use it:\n",
        "- You want a probabilistically sound model that considers all classes simultaneously.\n",
        "- You’re using solvers like lbfgs or saga that support multinomial loss.\n",
        "- You care about calibrated probabilities and better decision boundaries in multiclass settings\n",
        "\n",
        "Q. 20 :- How do we interpret coefficients in Logistic Regression?\n",
        "Ans :- Interpreting coefficients in logistic regression is all about understanding how each predictor affects the log odds of the outcome — and, by extension, the odds or probability of the event occurring.\n",
        "Step-by-step interpretation:\n",
        "1. Raw Coefficients (β)\n",
        "Each coefficient represents the change in the log odds of the outcome for a one-unit increase in the predictor, holding all other variables constant.\n",
        "Example:\n",
        "If ( \\beta = 0.7 ), then a one-unit increase in that predictor increases the log odds by 0.7.\n",
        "2. Exponentiated Coefficients (Odds Ratios)\n",
        "To make interpretation more intuitive, we exponentiate the coefficient:\n",
        "[ \\text{Odds Ratio} = e^{\\beta} ]\n",
        "- If ( e^{\\beta} > 1 ): the odds of the outcome increase\n",
        "- If ( e^{\\beta} < 1 ): the odds decrease\n",
        "- If ( e^{\\beta} = 1 ): no effect\n",
        "Example:\n",
        "If ( \\beta = 0.7 ), then ( e^{0.7} \\approx 2.01 ). That means the odds of the outcome are about twice as high for each one-unit increase in the predictor.\n",
        "3. Negative Coefficients\n",
        "A negative coefficient means the predictor decreases the log odds of the outcome. For example, ( \\beta = -0.5 ) implies ( e^{-0.5} \\approx 0.61 ), or a 39% decrease in the odds.\n",
        "4. Categorical Variables\n",
        "For binary variables (e.g., Male = 1, Female = 0), the coefficient tells you how the odds change when the variable is 1 vs 0. For multi-category variables, interpretation is relative to a reference category.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "fIFAMbYK-AQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Practical***"
      ],
      "metadata": {
        "id": "Bxt3TsfSHHYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 1 :- Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "# Ans :-\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load example dataset (you can replace this with your own dataset)\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the logistic regression model\n",
        "model = LogisticRegression(max_iter=10000)  # Increase max_iter to ensure convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate and print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression Model Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ixDSOLNHLd4",
        "outputId": "98d8ee2b-6e52-4c1c-97f2-e74709d01a7e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Model Accuracy: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 2 :- Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
        "# Ans :-\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load sample dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the logistic regression model with L1 regularization\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)  # 'liblinear' supports L1 penalty\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"L1-Regularized Logistic Regression Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kzQzaZcHdCX",
        "outputId": "d950b2f6-8c9d-4f4d-f4dc-0341125d37d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1-Regularized Logistic Regression Accuracy: 0.9561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 3 :-  Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "# Ans :-\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train logistic regression with L2 regularization (default)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Output results\n",
        "print(f\"L2-Regularized Logistic Regression Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nModel Coefficients:\")\n",
        "for feature, coef in zip(X.columns, model.coef_[0]):\n",
        "    print(f\"{feature}: {coef:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEW690jCH6Oc",
        "outputId": "aa97a4f3-99e8-4a0b-be48-577d121d4fdd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2-Regularized Logistic Regression Accuracy: 0.9561\n",
            "\n",
            "Model Coefficients:\n",
            "mean radius: 2.0998\n",
            "mean texture: 0.1325\n",
            "mean perimeter: -0.1035\n",
            "mean area: -0.0026\n",
            "mean smoothness: -0.1702\n",
            "mean compactness: -0.3798\n",
            "mean concavity: -0.6912\n",
            "mean concave points: -0.4081\n",
            "mean symmetry: -0.2351\n",
            "mean fractal dimension: -0.0236\n",
            "radius error: -0.0854\n",
            "texture error: 1.1225\n",
            "perimeter error: -0.3258\n",
            "area error: -0.0652\n",
            "smoothness error: -0.0237\n",
            "compactness error: 0.0596\n",
            "concavity error: 0.0045\n",
            "concave points error: -0.0428\n",
            "symmetry error: -0.0415\n",
            "fractal dimension error: 0.0143\n",
            "worst radius: 0.9663\n",
            "worst texture: -0.3771\n",
            "worst perimeter: -0.0586\n",
            "worst area: -0.0240\n",
            "worst smoothness: -0.3177\n",
            "worst compactness: -1.0044\n",
            "worst concavity: -1.5713\n",
            "worst concave points: -0.6935\n",
            "worst symmetry: -0.8410\n",
            "worst fractal dimension: -0.0931\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 4 :- Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "# Ans :-\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train logistic regression with Elastic Net regularization\n",
        "model = LogisticRegression(\n",
        "    penalty='elasticnet',\n",
        "    solver='saga',\n",
        "    l1_ratio=0.5,       # Mix of L1 and L2 (0 = Ridge, 1 = Lasso)\n",
        "    C=1.0,              # Inverse of regularization strength\n",
        "    max_iter=10000\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Output results\n",
        "print(f\"Elastic Net Logistic Regression Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQMZefMgIPym",
        "outputId": "e5e63c0a-0666-4a7b-c888-5905cfdf3b51"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elastic Net Logistic Regression Accuracy: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 5 :- Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "# Ans :-\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a multiclass dataset (Iris)\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with One-vs-Rest strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Multiclass Logistic Regression (OvR) Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vAEcFALIgYr",
        "outputId": "f06d1321-23f9-4fc5-c692-2ab907f0659b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multiclass Logistic Regression (OvR) Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 6 :- Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "# Ans :-\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = LogisticRegression(solver='liblinear', max_iter=1000)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Output results\n",
        "print(f\"Best Parameters: {grid.best_params_}\")\n",
        "print(f\"Test Set Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10weeZSCIw7f",
        "outputId": "eb2b348e-47c2-49e6-b3e0-025b113d560b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 100, 'penalty': 'l1'}\n",
            "Test Set Accuracy: 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 7 :- Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "# Ans :-\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Define the model\n",
        "model = LogisticRegression(max_iter=10000, solver='liblinear')\n",
        "\n",
        "# Set up Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Evaluate model using cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Print results\n",
        "print(f\"Cross-Validation Accuracies: {scores}\")\n",
        "print(f\"Average Accuracy: {scores.mean():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-tZtXcrJETg",
        "outputId": "2a5a9c45-a57a-4157-9abf-d3fd44e9196c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Accuracies: [0.94736842 0.92105263 0.95614035 0.96491228 0.96460177]\n",
            "Average Accuracy: 0.9508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 8 :- Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "#Ans :-\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from CSV\n",
        "# Replace 'your_dataset.csv' with your actual file path\n",
        "data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Separate features and target\n",
        "# Replace 'target_column' with the name of your target column\n",
        "X = data.drop('target_column', axis=1)\n",
        "y = data['target_column']\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train logistic regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Logistic Regression Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "-Oyg-MQyJVun",
        "outputId": "013634b1-cb93-49d0-f6f9-1ebfdb927410"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'your_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-3570069060.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load dataset from CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Replace 'your_dataset.csv' with your actual file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'your_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Separate features and target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9 :- Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "# Ans :-\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import loguniform\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Define hyperparameter distributions\n",
        "param_dist = {\n",
        "    'C': loguniform(0.001, 100),  # Continuous range for C\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']  # These solvers support both L1 and L2\n",
        "}\n",
        "\n",
        "# Set up RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Output results\n",
        "print(f\"Best Parameters: {random_search.best_params_}\")\n",
        "print(f\"Test Set Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSRu9zTpJsit",
        "outputId": "506a738c-e1c9-47a6-ecdb-a2b46fc45d54"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': np.float64(4.5705630998014515), 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Test Set Accuracy: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 10 :- Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "# Ans :-\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a multiclass dataset (Iris)\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Wrap Logistic Regression with One-vs-One strategy\n",
        "base_model = LogisticRegression(solver='liblinear', max_iter=1000)\n",
        "ovo_model = OneVsOneClassifier(base_model)\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"One-vs-One Logistic Regression Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAgAcA58KC2d",
        "outputId": "098b69a5-5ce3-4035-b2bd-92b53bbc1b4a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Logistic Regression Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 11 :- Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "# Ans :-\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=data.target_names,\n",
        "            yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "RoI8XfinLbvi",
        "outputId": "64d2eca9-5f43-4c07-ddfd-f04ee681fd7a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAikAAAGGCAYAAAC+MRG4AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQ0NJREFUeJzt3XlYVGX7B/DvAWFAlmFRtpJFMcDUVNxQcws1M0XBNVPc0gw3cIveXCvpNRWXXLIMzFxyzzL3DRdcUtwLcYtSFlMBQRmWOb8/fJ1fI5gzMMwcjt9P17ku5znLc5+5mry7n+c5RxBFUQQRERGRxJiZOgAiIiKi0jBJISIiIklikkJERESSxCSFiIiIJIlJChEREUkSkxQiIiKSJCYpREREJElMUoiIiEiSmKQQERGRJDFJIZKQlJQUdOzYEUqlEoIgYOvWrQa9/s2bNyEIAuLj4w163cqsbdu2aNu2ranDIKJSMEkhesq1a9cwYsQI1KxZE1ZWVrC3t0fLli2xYMECPHr0qEL7Dg8Px4ULF/DZZ59h1apVaNy4cYX2Z0yDBg2CIAiwt7cv9XtMSUmBIAgQBAFz5szR+/q3b9/G9OnTcfbsWQNES0RSUMXUARBJyfbt29GrVy8oFAoMHDgQdevWRUFBAY4cOYKJEyfi0qVLWL58eYX0/ejRIyQmJuI///kPRo0aVSF9eHl54dGjR7CwsKiQ6z9PlSpV8PDhQ/z000/o3bu31r7Vq1fDysoK+fn5Zbr27du3MWPGDHh7e6NBgwY6n7d79+4y9UdEFY9JCtH/3LhxA3379oWXlxf2798Pd3d3zb6IiAhcvXoV27dvr7D+79y5AwBwcHCosD4EQYCVlVWFXf95FAoFWrZsibVr15ZIUtasWYMuXbpg06ZNRonl4cOHqFq1KiwtLY3SHxHpj8M9RP8ze/Zs5ObmYsWKFVoJyhO+vr4YO3as5nNRURE++eQT1KpVCwqFAt7e3vjoo4+gUqm0zvP29sbbb7+NI0eOoGnTprCyskLNmjXx3XffaY6ZPn06vLy8AAATJ06EIAjw9vYG8HiY5Mmf/2n69OkQBEGrbc+ePWjVqhUcHBxga2sLPz8/fPTRR5r9z5qTsn//frz++uuwsbGBg4MDQkJC8Ntvv5Xa39WrVzFo0CA4ODhAqVRi8ODBePjw4bO/2Ke888472LFjB7KysjRtp06dQkpKCt55550Sx9+7dw8TJkxAvXr1YGtrC3t7e3Tu3Bnnzp3THHPw4EE0adIEADB48GDNsNGT+2zbti3q1q2L06dPo3Xr1qhatarme3l6Tkp4eDisrKxK3H+nTp3g6OiI27dv63yvRFQ+TFKI/uenn35CzZo10aJFC52OHzZsGKZOnYpGjRohNjYWbdq0QUxMDPr27Vvi2KtXr6Jnz57o0KED5s6dC0dHRwwaNAiXLl0CAISGhiI2NhYA0K9fP6xatQrz58/XK/5Lly7h7bffhkqlwsyZMzF37lx069YNR48e/dfz9u7di06dOiEzMxPTp09HVFQUjh07hpYtW+LmzZslju/duzcePHiAmJgY9O7dG/Hx8ZgxY4bOcYaGhkIQBGzevFnTtmbNGvj7+6NRo0Yljr9+/Tq2bt2Kt99+G/PmzcPEiRNx4cIFtGnTRpMwBAQEYObMmQCA4cOHY9WqVVi1ahVat26tuc7du3fRuXNnNGjQAPPnz0e7du1KjW/BggWoXr06wsPDUVxcDAD46quvsHv3bixatAgeHh463ysRlZNIRGJ2drYIQAwJCdHp+LNnz4oAxGHDhmm1T5gwQQQg7t+/X9Pm5eUlAhATEhI0bZmZmaJCoRDHjx+vabtx44YIQPziiy+0rhkeHi56eXmViGHatGniP3/CsbGxIgDxzp07z4z7SR9xcXGatgYNGoguLi7i3bt3NW3nzp0TzczMxIEDB5bob8iQIVrX7NGjh+js7PzMPv95HzY2NqIoimLPnj3FN954QxRFUSwuLhbd3NzEGTNmlPod5Ofni8XFxSXuQ6FQiDNnztS0nTp1qsS9PdGmTRsRgLhs2bJS97Vp00arbdeuXSIA8dNPPxWvX78u2trait27d3/uPRKRYbGSQgQgJycHAGBnZ6fT8b/88gsAICoqSqt9/PjxAFBi7kqdOnXw+uuvaz5Xr14dfn5+uH79epljftqTuSw//vgj1Gq1TuekpaXh7NmzGDRoEJycnDTt9evXR4cOHTT3+U/vv/++1ufXX38dd+/e1XyHunjnnXdw8OBBpKenY//+/UhPTy91qAd4PI/FzOzxf6qKi4tx9+5dzVDWmTNndO5ToVBg8ODBOh3bsWNHjBgxAjNnzkRoaCisrKzw1Vdf6dwXERkGkxQiAPb29gCABw8e6HT8H3/8ATMzM/j6+mq1u7m5wcHBAX/88YdWu6enZ4lrODo64v79+2WMuKQ+ffqgZcuWGDZsGFxdXdG3b1+sX7/+XxOWJ3H6+fmV2BcQEIC///4beXl5Wu1P34ujoyMA6HUvb731Fuzs7PDDDz9g9erVaNKkSYnv8gm1Wo3Y2FjUrl0bCoUC1apVQ/Xq1XH+/HlkZ2fr3OdLL72k1yTZOXPmwMnJCWfPnsXChQvh4uKi87lEZBhMUojwOEnx8PDAxYsX9Trv6Ymrz2Jubl5quyiKZe7jyXyJJ6ytrZGQkIC9e/diwIABOH/+PPr06YMOHTqUOLY8ynMvTygUCoSGhmLlypXYsmXLM6soADBr1ixERUWhdevW+P7777Fr1y7s2bMHr776qs4VI+Dx96OPpKQkZGZmAgAuXLig17lEZBhMUoj+5+2338a1a9eQmJj43GO9vLygVquRkpKi1Z6RkYGsrCzNSh1DcHR01FoJ88TT1RoAMDMzwxtvvIF58+bh8uXL+Oyzz7B//34cOHCg1Gs/iTM5ObnEvt9//x3VqlWDjY1N+W7gGd555x0kJSXhwYMHpU42fmLjxo1o164dVqxYgb59+6Jjx44IDg4u8Z3omjDqIi8vD4MHD0adOnUwfPhwzJ49G6dOnTLY9YlIN0xSiP5n0qRJsLGxwbBhw5CRkVFi/7Vr17BgwQIAj4crAJRYgTNv3jwAQJcuXQwWV61atZCdnY3z589r2tLS0rBlyxat4+7du1fi3CcPNXt6WfQT7u7uaNCgAVauXKn1l/7Fixexe/duzX1WhHbt2uGTTz7Bl19+CTc3t2ceZ25uXqJKs2HDBty6dUur7UkyVVpCp6/JkycjNTUVK1euxLx58+Dt7Y3w8PBnfo9EVDH4MDei/6lVqxbWrFmDPn36ICAgQOuJs8eOHcOGDRswaNAgAMBrr72G8PBwLF++HFlZWWjTpg1OnjyJlStXonv37s9c3loWffv2xeTJk9GjRw+MGTMGDx8+xNKlS/HKK69oTRydOXMmEhIS0KVLF3h5eSEzMxNLlizByy+/jFatWj3z+l988QU6d+6MoKAgDB06FI8ePcKiRYugVCoxffp0g93H08zMzPDxxx8/97i3334bM2fOxODBg9GiRQtcuHABq1evRs2aNbWOq1WrFhwcHLBs2TLY2dnBxsYGzZo1g4+Pj15x7d+/H0uWLMG0adM0S6Lj4uLQtm1bTJkyBbNnz9brekRUDiZeXUQkOVeuXBHfe+890dvbW7S0tBTt7OzEli1biosWLRLz8/M1xxUWFoozZswQfXx8RAsLC7FGjRpidHS01jGi+HgJcpcuXUr08/TS12ctQRZFUdy9e7dYt25d0dLSUvTz8xO///77EkuQ9+3bJ4aEhIgeHh6ipaWl6OHhIfbr10+8cuVKiT6eXqa7d+9esWXLlqK1tbVob28vdu3aVbx8+bLWMU/6e3qJc1xcnAhAvHHjxjO/U1HUXoL8LM9agjx+/HjR3d1dtLa2Flu2bCkmJiaWunT4xx9/FOvUqSNWqVJF6z7btGkjvvrqq6X2+c/r5OTkiF5eXmKjRo3EwsJCreMiIyNFMzMzMTEx8V/vgYgMRxBFPWa7ERERERkJ56QQERGRJDFJISIiIklikkJERESSxCSFiIiI9OLt7a152/g/t4iICABAfn4+IiIi4OzsDFtbW4SFhZX6aIfn4cRZIiIi0sudO3e0nmR98eJFdOjQAQcOHEDbtm0xcuRIbN++HfHx8VAqlRg1ahTMzMye+1b2pzFJISIionIZN24cfv75Z6SkpCAnJwfVq1fHmjVr0LNnTwCPn2AdEBCAxMRENG/eXOfrcriHiIiIoFKpkJOTo7Xp8pTlgoICfP/99xgyZAgEQcDp06dRWFiI4OBgzTH+/v7w9PTU6bUj/yTLJ86++/05U4dAJAuLQuuaOgQiWXCsWvqLOQ3NuuGoMp87OaQaZsyYodU2bdq05z55euvWrcjKytI8kTs9PR2WlpZwcHDQOs7V1RXp6el6xSTLJIWIiIj0Ex0djaioKK02hULx3PNWrFiBzp07w8PDw+AxMUkhIiKSC6HsszgUCoVOSck//fHHH9i7dy82b96saXNzc0NBQQGysrK0qikZGRn/+jLR0nBOChERkVwIQtm3MoiLi4OLi4vWm98DAwNhYWGBffv2adqSk5ORmpqKoKAgva7PSgoREZFclKOSoi+1Wo24uDiEh4ejSpX/TyeUSiWGDh2KqKgoODk5wd7eHqNHj0ZQUJBeK3sAJilERETyUcaKSFns3bsXqampGDJkSIl9sbGxMDMzQ1hYGFQqFTp16oQlS5bo3Ycsn5PC1T1EhsHVPUSGYbTVPU0nlPncRyfnGDASw2AlhYiISC6MWEkxBk6cJSIiIkliJYWIiEgujDhx1hiYpBAREcmFzIZ7mKQQERHJBSspREREJEmspBAREZEkyaySIq+7ISIiItlgJYWIiEguONxDREREkiSz4R4mKURERHLBJIWIiIgkyYzDPURERCRFMqukyOtuiIiISDZYSSEiIpILru4hIiIiSZLZcA+TFCIiIrlgJYWIiIgkiZUUIiIikiRWUoiIiEiSZFZJkdfdEBERkWywkkJERCQXHO4hIiIiSZLZcA+TFCIiIrlgJYWIiIgkiZUUIiIikiSZJSnyuhsiIiKSDVZSiIiI5IJzUoiIiEiSZDbcwySFiIhILlhJISIiIkliJYWIiIgkSWaVFHmlXERERCQbTFKIiIhkQhCEMm/6unXrFt599104OzvD2toa9erVw6+//qrZL4oipk6dCnd3d1hbWyM4OBgpKSl69cEkhYiISCaMlaTcv38fLVu2hIWFBXbs2IHLly9j7ty5cHR01Bwze/ZsLFy4EMuWLcOJEydgY2ODTp06IT8/X+d+OCeFiIhILow0JeW///0vatSogbi4OE2bj4+P5s+iKGL+/Pn4+OOPERISAgD47rvv4Orqiq1bt6Jv37469cNKChERkUyUp5KiUqmQk5OjtalUqlL72bZtGxo3boxevXrBxcUFDRs2xNdff63Zf+PGDaSnpyM4OFjTplQq0axZMyQmJup8P0xSiIiIZKI8SUpMTAyUSqXWFhMTU2o/169fx9KlS1G7dm3s2rULI0eOxJgxY7By5UoAQHp6OgDA1dVV6zxXV1fNPl1IYrjH3NwcaWlpcHFx0Wq/e/cuXFxcUFxcbKLIiIiIXgzR0dGIiorSalMoFKUeq1ar0bhxY8yaNQsA0LBhQ1y8eBHLli1DeHi4wWKSRCVFFMVS21UqFSwtLY0cDRERUeVUnkqKQqGAvb291vasJMXd3R116tTRagsICEBqaioAwM3NDQCQkZGhdUxGRoZmny5MWklZuHAhgMdf6jfffANbW1vNvuLiYiQkJMDf399U4REREVUqZVlKXBYtW7ZEcnKyVtuVK1fg5eUF4PEkWjc3N+zbtw8NGjQAAOTk5ODEiRMYOXKkzv2YNEmJjY0F8LiSsmzZMpibm2v2WVpawtvbG8uWLTNVeERERJWLkVb3REZGokWLFpg1axZ69+6NkydPYvny5Vi+fPnjMAQB48aNw6efforatWvDx8cHU6ZMgYeHB7p3765zPyZNUm7cuAEAaNeuHTZv3qy1vpqIiIj0Y6xKSpMmTbBlyxZER0dj5syZ8PHxwfz589G/f3/NMZMmTUJeXh6GDx+OrKwstGrVCjt37oSVlZXO/QjisyaEVGLvfn/O1CEQycKi0LqmDoFIFhyrmj//IEP08+7qMp97//v+zz/IyCSxuqe4uBjx8fHYt28fMjMzoVartfbv37/fRJERERFVHsaqpBiLJJKUsWPHIj4+Hl26dEHdunVl9yUTERGR/iSRpKxbtw7r16/HW2+9ZepQiIiIKi25/U++JJIUS0tL+Pr6mjoMIiKiyk1eOYo0HuY2fvx4LFiw4JkPdSMiIqLnM9ZbkI1FEpWUI0eO4MCBA9ixYwdeffVVWFhYaO3fvHmziSIjIiKqPKSabJSVJJIUBwcH9OjRw9RhEBERVWpMUipAXFycqUMgIiIiiZFEkkJEREQGIK9CinSSlI0bN2L9+vVITU1FQUGB1r4zZ86YKCoiIqLKQ27DPZJY3bNw4UIMHjwYrq6uSEpKQtOmTeHs7Izr16+jc+fOpg6PiIioUpDb6h5JJClLlizB8uXLsWjRIlhaWmLSpEnYs2cPxowZg+zsbFOHR0REVCkwSakAqampaNGiBQDA2toaDx48AAAMGDAAa9euNWVoRERElQaTlArg5uaGe/fuAQA8PT1x/PhxAMCNGzf4gDciIqIXlCSSlPbt22Pbtm0AgMGDByMyMhIdOnRAnz59+PwUIiIiXQnl2CRIEqt7li9fDrVaDQCIiIiAs7Mzjh07hm7dumHEiBEmjo6IiKhykOqwTVlJIkkxMzODmdn/F3X69u2Lvn37mjAiIiKiyodJSgXJysrCyZMnkZmZqamqPDFw4EATRUVERFR5MEmpAD/99BP69++P3Nxc2Nvba33JgiAwSSEiItKFvHIUaSQp48ePx5AhQzBr1ixUrVrV1OGQgbxR2xlvvOKM6jaWAIC/svOx5UIGzt9+vMTcxdYS7zTywCsuNrAwE3A+7QFWnrqFnPwiU4ZNVOl89+3XWLIoFn3eGYDIidGmDodMSG6VFEms7rl16xbGjBnDBEVm7j0sxA9Jafh4xxVM2XEFl9NzEdXGGy8pFVCYm2HyGzUhQsSsvdcwY/dVmJsJGN/WR27/I0BUoS5fuoAtm9bDt7afqUMhMjhJJCmdOnXCr7/+auowyMCSbuXg3O0HyHhQgPQHBdhwLh35RWr4VrNBbZeqqG5jieWJf+KvrHz8lZWPr46lwsfZGnXcbE0dOlGl8PBhHqZ9NAnRU2bAzt7e1OGQBMjtYW6SGO7p0qULJk6ciMuXL6NevXqwsLDQ2t+tWzcTRUaGIghAM08HKKqYIeXvPLjaKiACKCz+/4f1FRaLEEXAz8UGl9JzTRcsUSUxJ+ZTtHy9DZo2b4G4b74ydTgkAVJNNspKEknKe++9BwCYOXNmiX2CIKC4uNjYIZGBvOxghemdfGFhbob8IjXmH7qJ29kqPMgvgqpIjb4N3bH+bBoECOjT0B3mZgIcrC2ef2GiF9yenb8g+ffL+Pb79aYOhSSESUoFeHrJsT5UKhVUKpVWW3FhAcwtLMsbFhlAWo4K/9l+BdaW5mjqqcSIFp74dM9V3M5WYeHhmxjc9GV09K8GUQQSb97HjbsPoearEIj+VUZ6GuZ9EYOFS7+BQqEwdTgkJfLKUaSRpJRHTEwMZsyYodVWr8cI1A8daaKI6J+K1SIycgsAADfvPUJN56p40786vj3xFy6m5WL8j7/DVmEOtVrEw0I1vgyrgzt/FJg4aiJp+/23S7h/7y4GvdNT01ZcXIyzZ37Fxh/WIOHEWZibm5swQjIVVlIqwMKFC0ttFwQBVlZW8PX1RevWrUv90UVHRyMqKkqrbcSm5AqJk8pPEIAqZto/olzV4+G8Oq62sLeqgjN/5ZgiNKJKo3HTIKze8KNW26fT/gMvHx8MGDSMCQrJhiSSlNjYWNy5cwcPHz6Eo6MjAOD+/fuoWrUqbG1tkZmZiZo1a+LAgQOoUaOG1rkKhaJEuZNDPdLQu4Ebzt1+gLt5BbCyMEcLbwcEuNpi9r7rAIDWNR1xK+fx/JTa1avi3cYvYedvd5CWo3rOlYlebDY2NqjlW1urzcraGkqlQ4l2erHIrZIiiSXIs2bNQpMmTZCSkoK7d+/i7t27uHLlCpo1a4YFCxYgNTUVbm5uiIyMNHWopAd7qyp4v4Unvujmj+jgmqjpXBWz913Hxf+t3HG3t0JkG2/M7uqH7vXcsO1iBtacSTNx1ERElZcglH2TIkEUTT9LsVatWti0aRMaNGig1Z6UlISwsDBcv34dx44dQ1hYGNLSnv+X2Lvfn6ugSIleLItC65o6BCJZcKxqnCG42hN3lvnclC/eNGAkhiGJ4Z60tDQUFZV8FHpRURHS09MBAB4eHnjw4IGxQyMiIqo0pFoRKStJDPe0a9cOI0aMQFJSkqYtKSkJI0eORPv27QEAFy5cgI+Pj6lCJCIikjy5PXFWEknKihUr4OTkhMDAQM1E2MaNG8PJyQkrVqwAANja2mLu3LkmjpSIiIiMRRJJipubG/bs2YPLly9jw4YN2LBhAy5fvozdu3fD1dUVwONqS8eOHU0cKRERkXQZa+Ls9OnTS1Ri/P39Nfvz8/MREREBZ2dn2NraIiwsDBkZGXrfjyTmpDzh7++vdZNERESkOzMz4w3bvPrqq9i7d6/mc5Uq/59SREZGYvv27diwYQOUSiVGjRqF0NBQHD16VK8+TJakREVF4ZNPPoGNjU2Jh7E9bd68eUaKioiIqPIy5tSSKlWqwM3NrUR7dnY2VqxYgTVr1mjmlcbFxSEgIADHjx9H8+bNde/DYNHqKSkpCYWFhZo/P4tUJ/MQERFJjTH/zkxJSYGHhwesrKwQFBSEmJgYeHp64vTp0ygsLERwcLDmWH9/f3h6eiIxMbFyJCkHDhwo9c9ERERUNuXJUUp7YW9pT3UHgGbNmiE+Ph5+fn5IS0vDjBkz8Prrr+PixYtIT0+HpaUlHBwctM5xdXXVPFZEV5KYOEtERESmFRMTA6VSqbXFxMSUemznzp3Rq1cv1K9fH506dcIvv/yCrKwsrF+/3qAxmaySEhoaqvOxmzdvrsBIiIiI5KE8wz2lvbC3tCpKaRwcHPDKK6/g6tWr6NChAwoKCpCVlaVVTcnIyCh1Dsu/MVmSolQqTdU1ERGRLJUnSXnW0I4ucnNzce3aNQwYMACBgYGwsLDAvn37EBYWBgBITk5GamoqgoKC9LquyZKUuLg4U3VNREQkS8aaNzthwgR07doVXl5euH37NqZNmwZzc3P069cPSqUSQ4cORVRUFJycnGBvb4/Ro0cjKChIr0mzgMSek0JERERlZ6zVPX/99Rf69euHu3fvonr16mjVqhWOHz+O6tWrAwBiY2NhZmaGsLAwqFQqdOrUCUuWLNG7H8kkKRs3bsT69euRmpqKgoICrX1nzpwxUVRERESVh7EqKevWrfvX/VZWVli8eDEWL15crn4ksbpn4cKFGDx4MFxdXZGUlISmTZvC2dkZ169fR+fOnU0dHhERUaXAFwxWgCVLlmD58uVYtGgRLC0tMWnSJOzZswdjxoxBdna2qcMjIiIiE5BEkpKamooWLVoAAKytrfHgwQMAwIABA7B27VpThkZERFRpGOsFg8YiiSTFzc0N9+7dAwB4enri+PHjAIAbN25AFEVThkZERFRpcLinArRv3x7btm0DAAwePBiRkZHo0KED+vTpgx49epg4OiIiospBbpUUSazuWb58OdRqNQAgIiIC1apVw9GjR9GtWze8//77Jo6OiIiocpBqRaSsJJGkmJmZoaCgAGfOnEFmZiasra01b0/cuXMnunbtauIIiYiIpE9mOYo0kpSdO3diwIABuHv3bol9giCguLjYBFERERGRKUliTsro0aPRu3dvpKWlQa1Wa21MUIiIiHQjt4mzkqikZGRkICoqCq6urqYOhYiIqNKSaK5RZpKopPTs2RMHDx40dRhERESVGispFeDLL79Er169cPjwYdSrVw8WFhZa+8eMGWOiyIiIiCoPieYaZSaJJGXt2rXYvXs3rKyscPDgQa2MThAEJilEREQ6kGpFpKwkkaT85z//wYwZM/Dhhx/CzEwSI1BERERkYpJIUgoKCtCnTx8mKEREROUgt0qKJLKC8PBw/PDDD6YOg4iIqFLjY/ErQHFxMWbPno1du3ahfv36JSbOzps3z0SRERERVR5yq6RIIkm5cOECGjZsCAC4ePGi1j65feFEREQVRW5/ZUoiSTlw4ICpQyAiIqr05PY/9pJIUoiIiKj8ZJajSGPiLBEREdHTWEkhIiKSCTOZlVKYpBAREcmEzHIUJilERERywYmzREREJElm8spRmKQQERHJhdwqKVzdQ0RERJLESgoREZFMyKyQwiSFiIhILgTIK0thkkJERCQTnDhLREREkiS3ibNMUoiIiGRCZjkKV/cQERGRNDFJISIikgkzQSjzVh6ff/45BEHAuHHjNG35+fmIiIiAs7MzbG1tERYWhoyMDP3up1xRERERkWQIQtm3sjp16hS++uor1K9fX6s9MjISP/30EzZs2IBDhw7h9u3bCA0N1evaTFKIiIhkQhCEMm9lkZubi/79++Prr7+Go6Ojpj07OxsrVqzAvHnz0L59ewQGBiIuLg7Hjh3D8ePHdb4+kxQiIiKZMHYlJSIiAl26dEFwcLBW++nTp1FYWKjV7u/vD09PTyQmJup8fa7uISIikonyzC1RqVRQqVRabQqFAgqFotTj161bhzNnzuDUqVMl9qWnp8PS0hIODg5a7a6urkhPT9c5Jp2SlG3btul8wW7duul8LBEREUlDTEwMZsyYodU2bdo0TJ8+vcSxf/75J8aOHYs9e/bAysqqwmLSKUnp3r27ThcTBAHFxcXliYeIiIjKqDxrdKKjoxEVFaXV9qwqyunTp5GZmYlGjRpp2oqLi5GQkIAvv/wSu3btQkFBAbKysrSqKRkZGXBzc9M5Jp2SFLVarfMFiYiIyDTK88TZfxvaedobb7yBCxcuaLUNHjwY/v7+mDx5MmrUqAELCwvs27cPYWFhAIDk5GSkpqYiKChI55g4J4WIiEgmjPXuHjs7O9StW1erzcbGBs7Ozpr2oUOHIioqCk5OTrC3t8fo0aMRFBSE5s2b69xPmZKUvLw8HDp0CKmpqSgoKNDaN2bMmLJckoiIiMpJSu/uiY2NhZmZGcLCwqBSqdCpUycsWbJEr2sIoiiK+pyQlJSEt956Cw8fPkReXh6cnJzw999/o2rVqnBxccH169f1CqAivPv9OVOHQCQLi0LrPv8gInoux6rmRulnwOqy//23qv9rBozEMPR+TkpkZCS6du2K+/fvw9raGsePH8cff/yBwMBAzJkzpyJiJCIiIh0Y+2FuFU3vJOXs2bMYP348zMzMYG5uDpVKhRo1amD27Nn46KOPKiJGIiIiegHpnaRYWFjAzOzxaS4uLkhNTQUAKJVK/Pnnn4aNjoiIiHRmJpR9kyK9J842bNgQp06dQu3atdGmTRtMnToVf//9N1atWlVipi8REREZj1SHbcpK70rKrFmz4O7uDgD47LPP4OjoiJEjR+LOnTtYvny5wQMkIiIi3Qjl2KRI70pK48aNNX92cXHBzp07DRoQERERlU153t0jRXyYGxERkUzILEfRP0nx8fH51zEvKTwnhYiIiCo/vZOUcePGaX0uLCxEUlISdu7ciYkTJxoqLiIiItKT3CbO6p2kjB07ttT2xYsX49dffy13QERERFQ2MstR9F/d8yydO3fGpk2bDHU5IiIi0pOZIJR5kyKDTZzduHEjnJycDHU5IiIi0pNEc40yK9PD3P455iWKItLT03Hnzh29325IREREhvPCz0kJCQnR+hLMzMxQvXp1tG3bFv7+/gYNjoiIiF5cgiiKoqmDMLT8IlNHQCQPjk1GmToEIll4lPSlUfoZveW3Mp+7qEeAASMxDL0nzpqbmyMzM7NE+927d2Fubm6QoIiIiEh/giCUeZMivYd7nlV4UalUsLS0LHdAREREVDZSfZtxWemcpCxcuBDA4yztm2++ga2trWZfcXExEhISOCeFiIjIhF7YJCU2NhbA40rKsmXLtIZ2LC0t4e3tjWXLlhk+QiIiItKJVIdtykrnJOXGjRsAgHbt2mHz5s1wdHSssKCIiIhIfy9sJeWJAwcOVEQcRERERFr0Xt0TFhaG//73vyXaZ8+ejV69ehkkKCIiItKfIJR9kyK9k5SEhAS89dZbJdo7d+6MhIQEgwRFRERE+nvh392Tm5tb6lJjCwsL5OTkGCQoIiIi0p/B3hosEXrfT7169fDDDz+UaF+3bh3q1KljkKCIiIhIf3Ib7tG7kjJlyhSEhobi2rVraN++PQBg3759WLNmDTZu3GjwAImIiEg3Uh22KSu9k5SuXbti69atmDVrFjZu3Ahra2u89tpr2L9/P5ycnCoiRiIiInoB6Z2kAECXLl3QpUsXAEBOTg7Wrl2LCRMm4PTp0yguLjZogERERKQbmRVSyj7HJiEhAeHh4fDw8MDcuXPRvn17HD9+3JCxERERkR7MhLJvUqRXJSU9PR3x8fFYsWIFcnJy0Lt3b6hUKmzdupWTZomIiExMbnNSdK6kdO3aFX5+fjh//jzmz5+P27dvY9GiRRUZGxEREenhhV3ds2PHDowZMwYjR45E7dq1KzImIiIiKgOpDtuUlc6VlCNHjuDBgwcIDAxEs2bN8OWXX+Lvv/+uyNiIiIjoBaZzktK8eXN8/fXXSEtLw4gRI7Bu3Tp4eHhArVZjz549ePDgQUXGSURERM8hlOMffSxduhT169eHvb097O3tERQUhB07dmj25+fnIyIiAs7OzrC1tUVYWBgyMjL0vh+9V/fY2NhgyJAhOHLkCC5cuIDx48fj888/h4uLC7p166Z3AERERGQYxlrd8/LLL+Pzzz/H6dOn8euvv6J9+/YICQnBpUuXAACRkZH46aefsGHDBhw6dAi3b99GaGio3vcjiKIo6n3WU4qLi/HTTz/h22+/xbZt28p7uXLLLzJ1BETy4NhklKlDIJKFR0lfGqWf2QeulfncSe1qlatvJycnfPHFF+jZsyeqV6+ONWvWoGfPngCA33//HQEBAUhMTETz5s11vqZB3kVkbm6O7t27SyJBISIielEJglDmrayKi4uxbt065OXlISgoCKdPn0ZhYSGCg4M1x/j7+8PT0xOJiYl6XbtMT5wlIiIi6SnP6h6VSgWVSqXVplAooFAoSj3+woULCAoKQn5+PmxtbbFlyxbUqVMHZ8+ehaWlJRwcHLSOd3V1RXp6ul4xye2tzkRERC+s8jwnJSYmBkqlUmuLiYl5Zl9+fn44e/YsTpw4gZEjRyI8PByXL1826P2wkkJERESIjo5GVFSUVtuzqigAYGlpCV9fXwBAYGAgTp06hQULFqBPnz4oKChAVlaWVjUlIyMDbm5uesXESgoREZFMmAlCmTeFQqFZUvxk+7ck5WlqtRoqlQqBgYGwsLDAvn37NPuSk5ORmpqKoKAgve6HlRQiIiKZMNYTZ6Ojo9G5c2d4enriwYMHWLNmDQ4ePIhdu3ZBqVRi6NChiIqKgpOTE+zt7TF69GgEBQXptbIHYJJCREQkG8Z6B09mZiYGDhyItLQ0KJVK1K9fH7t27UKHDh0AALGxsTAzM0NYWBhUKhU6deqEJUuW6N2PQZ6TIjV8TgqRYfA5KUSGYaznpCw+erPM50a09DZYHIbCSgoREZFMSPVtxmXFibNEREQkSaykEBERyYSxJs4aC5MUIiIimTCT2XgPkxQiIiKZkFmOwiSFiIhILlhJISIiIkmSWY7C1T1EREQkTaykEBERyYTcKg9MUoiIiGRCkNl4D5MUIiIimZBXisIkhYiISDa4uoeIiIgkSV4pivzm2BAREZFMsJJCREQkEzIb7WGSQkREJBdc3UNERESSJLc5HExSiIiIZIKVFCIiIpIkeaUoTFKIiIhkQ26VFLkNXxEREZFMsJJCREQkE3KrPDBJISIikgm5DfcwSSEiIpIJeaUoTFKIiIhkQ2aFFOkkKSkpKThw4AAyMzOhVqu19k2dOtVEUREREVUeZjKrpUgiSfn6668xcuRIVKtWDW5ublpjaoIgMEkhIiJ6AUkiSfn000/x2WefYfLkyaYOhYiIqNLicE8FuH//Pnr16mXqMIiIiCo1QWbDPZJYUt2rVy/s3r3b1GEQERFVaoJQ9k2KJFFJ8fX1xZQpU3D8+HHUq1cPFhYWWvvHjBljosiIiIgqD7lNnBVEURRNHYSPj88z9wmCgOvXr+t1vfyi8kZERADg2GSUqUMgkoVHSV8apZ9dl++U+dxOdaobMBLDkEQl5caNG6YOgYiIiCRGEnNSiIiIqPyMNSclJiYGTZo0gZ2dHVxcXNC9e3ckJydrHZOfn4+IiAg4OzvD1tYWYWFhyMjI0KsfSVRSoqKiSm0XBAFWVlbw9fVFSEgInJycjBwZERFR5WGs1T2HDh1CREQEmjRpgqKiInz00Ufo2LEjLl++DBsbGwBAZGQktm/fjg0bNkCpVGLUqFEIDQ3F0aNHde5HEnNS2rVrhzNnzqC4uBh+fn4AgCtXrsDc3Bz+/v5ITk6GIAg4cuQI6tSp89zrcU4KkWFwTgqRYRhrTsq+3/8u87lv+Fcr87l37tyBi4sLDh06hNatWyM7OxvVq1fHmjVr0LNnTwDA77//joCAACQmJqJ58+Y6XVcSwz0hISEIDg7G7du3cfr0aZw+fRp//fUXOnTogH79+uHWrVto3bo1IiMjTR0qERGRZAnl+Kc8srOzAUAz4nH69GkUFhYiODhYc4y/vz88PT2RmJio83UlMdzzxRdfYM+ePbC3t9e0KZVKTJ8+HR07dsTYsWMxdepUdOzY0YRREhERSVt5nneiUqmgUqm02hQKBRQKxb+ep1arMW7cOLRs2RJ169YFAKSnp8PS0hIODg5ax7q6uiI9PV3nmCRRScnOzkZmZmaJ9jt37iAnJwcA4ODggIKCAmOHRkREVGmUp5ISExMDpVKptcXExDy3z4iICFy8eBHr1q0z+P1IopISEhKCIUOGYO7cuWjSpAkA4NSpU5gwYQK6d+8OADh58iReeeUVE0ZJREQkX9HR0SUWsjyvijJq1Cj8/PPPSEhIwMsvv6xpd3NzQ0FBAbKysrSqKRkZGXBzc9M5JkkkKV999RUiIyPRt29fFBU9nvVapUoVhIeHIzY2FsDjsaxvvvnGlGGSAZz+9RTiv12B3y5fxJ07dxC7cDHavxH8/BOJXmC/b58BLw/nEu3LfkhA5OfrobCsgs+jQtGrUyAUllWwN/E3jJ31AzLvPTBBtGRKZuUY7tFlaOcJURQxevRobNmyBQcPHizxUNbAwEBYWFhg3759CAsLAwAkJycjNTUVQUFBOsckiSTF1tYWX3/9NWJjYzVPl61ZsyZsbW01xzRo0MBE0ZEhPXr0EH5+fugeGoaosVw5QqSLVu9+AfN//O1Tx9cDvywbjc17kgAAsyeEoXOrV9F/0grk5D5C7Ie9sW7uMLQfHGuqkMlEjLUEOSIiAmvWrMGPP/4IOzs7zTwTpVIJa2trKJVKDB06FFFRUXBycoK9vT1Gjx6NoKAgnVf2ABJJUp6wtbVF/fr1TR0GVaBWr7dBq9fbmDoMokrl7/u5Wp8nDK6La6l3cPh0CuxtrTCoexAGfRSPQ6euAACGT/se57ZMQdN63jh54aYJIiZTMdaLApcuXQoAaNu2rVZ7XFwcBg0aBACIjY2FmZkZwsLCoFKp0KlTJyxZskSvfkyWpISGhiI+Ph729vYIDQ3912M3b95spKiIiKTNooo5+r7VBAu/3w8AaBjgCUuLKth//P+f9nnlZgZS0+6hWX0fJikvGGO9XlCXR6xZWVlh8eLFWLx4cZn7MVmSolQqIfwv5VMqlaYKg4ioUunWrj4c7Kzx/U8nAABuzvZQFRQiO/eR1nGZd3Pg6mxf2iVIxsyMVUoxEpMlKXFxcaX+WV+lresWzXWf/ENEVJmEd2+BXUcvI+1OtqlDIapwknhOSnmUtq77i/8+f103EVFl4+nuiPbN/BC/9ZimLf1uDhSWFlDaWmsd6+Jsj4y7OcYOkUxMKMcmRZJIUjIyMjBgwAB4eHigSpUqMDc319r+TXR0NLKzs7W2iZOjjRQ5EZHxDOgWhMx7D7Dj8CVNW9JvqSgoLEK7Zn6attpeLvB0d8KJ8zdMESaZksyyFEms7hk0aBBSU1MxZcoUuLu7a+aq6KK0dd18waB0PczLQ2pqqubzrb/+wu+//QalUgl3Dw8TRkYkbYIgYGBIc6z++QSKi9Wa9pzcfMRvTcR/x4fiXnYeHuTlY97kXjh+7jonzb6AjLUE2VgkkaQcOXIEhw8f5rNQXgCXLl3EsMEDNZ/nzH48NNctpAc+mfW5qcIikrz2zfzg6e6ElVuPl9g3ac4mqNUi1s4Z9vhhbsd+w9iYH0wQJZmazObNQhB1WUdUwerUqYPVq1ejYcOGBrkeKylEhuHYhA/cIzKER0lfGqWfU9fLPqG6SU3prbSVxJyU+fPn48MPP8TNmzdNHQoRERFJhCSGe/r06YOHDx+iVq1aqFq1KiwsLLT237t3z0SRERERVSIyG+6RRJIyf/58U4dARERU6XHibAUIDw83dQhERESVntwmzkpiTgoAXLt2DR9//DH69euHzMxMAMCOHTtw6dKl55xJREREgOwekyKNJOXQoUOoV68eTpw4gc2bNyM39/EbP8+dO4dp06aZODoiIqJKQmZZiiSSlA8//BCffvop9uzZA0tLS017+/btcfx4yWcCEBERkfxJYk7KhQsXsGbNmhLtLi4u+Pvvv00QERERUeUjt4mzkqikODg4IC0trUR7UlISXnrpJRNEREREVPkIQtk3KZJEktK3b19MnjwZ6enpEAQBarUaR48exYQJEzBw4MDnX4CIiIjkNiVFGknKrFmz4O/vjxo1aiA3Nxd16tTB66+/jhYtWuDjjz82dXhERESVg8yyFEm8u+eJP//8ExcuXEBeXh4aNmwIX1/fMl2H7+4hMgy+u4fIMIz17p7zf+aW+dz6NWwNGIlhSGLiLACsWLECsbGxSElJAQDUrl0b48aNw7Bhw0wcGRERUeUg1bklZSWJJGXq1KmYN28eRo8ejaCgIABAYmIiIiMjkZqaipkzZ5o4QiIiIjI2SQz3VK9eHQsXLkS/fv202teuXYvRo0frvQyZwz1EhsHhHiLDMNZwz8W/yj7cU/dlDveUqrCwEI0bNy7RHhgYiKIiZhxEREQ6kdlwjyRW9wwYMABLly4t0b58+XL079/fBBERERFVPkI5/pEik1VSoqKiNH8WBAHffPMNdu/ejebNmwMATpw4gdTUVD4nhYiISEecOGsgSUlJWp8DAwMBPH4bMgBUq1YN1apV41uQiYiIdCSzHMV0ScqBAwdM1TURERFVApKYOEtEREQGILNSCpMUIiIimZDqBNiyYpJCREQkE5w4S0RERJIksxyFSQoREZFsyCxLkcTD3IiIiIiexkoKERGRTMht4iwrKURERDIhCGXf9JGQkICuXbvCw8MDgiBg69atWvtFUcTUqVPh7u4Oa2trBAcHIyUlRe/7YZJCREQkE0I5Nn3k5eXhtddew+LFi0vdP3v2bCxcuBDLli3DiRMnYGNjg06dOiE/P1+vfjjcQ0REJBdGGu3p3LkzOnfuXOo+URQxf/58fPzxxwgJCQEAfPfdd3B1dcXWrVvRt29fnfthJYWIiEgmyvMWZJVKhZycHK1NpVLpHcONGzeQnp6O4OBgTZtSqUSzZs2QmJio17WYpBAREclEeeakxMTEQKlUam0xMTF6x5Ceng4AcHV11Wp3dXXV7NMVh3uIiIgI0dHRiIqK0mpTKBQmiuYxJilEREQyUZ4pKQqFwiBJiZubGwAgIyMD7u7umvaMjAw0aNBAr2txuIeIiEgujLW851/4+PjAzc0N+/bt07Tl5OTgxIkTCAoK0utarKQQERHJhLEe5pabm4urV69qPt+4cQNnz56Fk5MTPD09MW7cOHz66aeoXbs2fHx8MGXKFHh4eKB79+569cMkhYiISCaM9RbkX3/9Fe3atdN8fjKXJTw8HPHx8Zg0aRLy8vIwfPhwZGVloVWrVti5cyesrKz06kcQRVE0aOQSkF9k6giI5MGxyShTh0AkC4+SvjRKP3/e03/J8BM1nEw7SbY0nJNCREREksThHiIiIpkw1nCPsTBJISIikg15ZSlMUoiIiGSClRQiIiKSJJnlKExSiIiI5EJulRSu7iEiIiJJYiWFiIhIJoz1xFljYZJCREQkF/LKUZikEBERyYXMchQmKURERHIht4mzTFKIiIhkQm5zUri6h4iIiCSJlRQiIiK5kFchhUkKERGRXMgsR2GSQkREJBecOEtERESSJLeJs0xSiIiIZEJulRSu7iEiIiJJYpJCREREksThHiIiIpmQ23APkxQiIiKZ4MRZIiIikiRWUoiIiEiSZJajMEkhIiKSDZllKVzdQ0RERJLESgoREZFMcOIsERERSRInzhIREZEkySxHYZJCREQkGzLLUpikEBERyYTc5qRwdQ8RERFJEispREREMiG3ibOCKIqiqYOgF49KpUJMTAyio6OhUChMHQ5RpcTfEckdkxQyiZycHCiVSmRnZ8Pe3t7U4RBVSvwdkdxxTgoRERFJEpMUIiIikiQmKURERCRJTFLIJBQKBaZNm8bJfkTlwN8RyR0nzhIREZEksZJCREREksQkhYiIiCSJSQoZxKBBg9C9e3fN57Zt22LcuHEmi4dIaozxm3j6d0hU2fGx+FQhNm/eDAsLC1OHUSpvb2+MGzeOSRTJzoIFC8BphiQnTFKoQjg5OZk6BKIXjlKpNHUIRAbF4Z4XUNu2bTF69GiMGzcOjo6OcHV1xddff428vDwMHjwYdnZ28PX1xY4dOwAAxcXFGDp0KHx8fGBtbQ0/Pz8sWLDguX38s1KRlpaGLl26wNraGj4+PlizZg28vb0xf/58zTGCIOCbb75Bjx49ULVqVdSuXRvbtm3T7Ncljifl7jlz5sDd3R3Ozs6IiIhAYWGhJq4//vgDkZGREAQBgtzexkWSVlRUhFGjRkGpVKJatWqYMmWKpvKhUqkwYcIEvPTSS7CxsUGzZs1w8OBBzbnx8fFwcHDArl27EBAQAFtbW7z55ptIS0vTHPP0cM+DBw/Qv39/2NjYwN3dHbGxsSV+m97e3pg1axaGDBkCOzs7eHp6Yvny5RX9VRDphEnKC2rlypWoVq0aTp48idGjR2PkyJHo1asXWrRogTNnzqBjx44YMGAAHj58CLVajZdffhkbNmzA5cuXMXXqVHz00UdYv369zv0NHDgQt2/fxsGDB7Fp0yYsX74cmZmZJY6bMWMGevfujfPnz+Ott95C//79ce/ePQDQOY4DBw7g2rVrOHDgAFauXIn4+HjEx8cDeDwM9fLLL2PmzJlIS0vT+g88UUVbuXIlqlSpgpMnT2LBggWYN28evvnmGwDAqFGjkJiYiHXr1uH8+fPo1asX3nzzTaSkpGjOf/jwIebMmYNVq1YhISEBqampmDBhwjP7i4qKwtGjR7Ft2zbs2bMHhw8fxpkzZ0ocN3fuXDRu3BhJSUn44IMPMHLkSCQnJxv+CyDSl0gvnDZt2oitWrXSfC4qKhJtbGzEAQMGaNrS0tJEAGJiYmKp14iIiBDDwsI0n8PDw8WQkBCtPsaOHSuKoij+9ttvIgDx1KlTmv0pKSkiADE2NlbTBkD8+OOPNZ9zc3NFAOKOHTueeS+lxeHl5SUWFRVp2nr16iX26dNH89nLy0urXyJjaNOmjRgQECCq1WpN2+TJk8WAgADxjz/+EM3NzcVbt25pnfPGG2+I0dHRoiiKYlxcnAhAvHr1qmb/4sWLRVdXV83nf/4Oc3JyRAsLC3HDhg2a/VlZWWLVqlU1v01RfPx7ePfddzWf1Wq16OLiIi5dutQg901UHpyT8oKqX7++5s/m5uZwdnZGvXr1NG2urq4AoKl2LF68GN9++y1SU1Px6NEjFBQUoEGDBjr1lZycjCpVqqBRo0aaNl9fXzg6Ov5rXDY2NrC3t9equOgSx6uvvgpzc3PNZ3d3d1y4cEGnWIkqUvPmzbWGGIOCgjB37lxcuHABxcXFeOWVV7SOV6lUcHZ21nyuWrUqatWqpfns7u5eakUSAK5fv47CwkI0bdpU06ZUKuHn51fi2H/+7gRBgJub2zOvS2RMTFJeUE+vvBEEQavtyX9I1Wo11q1bhwkTJmDu3LkICgqCnZ0dvvjiC5w4ccIocanVagDQOY5/uwaRFOXm5sLc3BynT5/WSrABwNbWVvPn0v7dFg2wmoe/GZIqJin0XEePHkWLFi3wwQcfaNquXbum8/l+fn4oKipCUlISAgMDAQBXr17F/fv3jRrHE5aWliguLtb7PKLyejqhPn78OGrXro2GDRuiuLgYmZmZeP311w3SV82aNWFhYYFTp07B09MTAJCdnY0rV66gdevWBumDqKJx4iw9V+3atfHrr79i165duHLlCqZMmYJTp07pfL6/vz+Cg4MxfPhwnDx5EklJSRg+fDisra31Wl1T3jie8Pb2RkJCAm7duoW///5b7/OJyio1NRVRUVFITk7G2rVrsWjRIowdOxavvPIK+vfvj4EDB2Lz5s24ceMGTp48iZiYGGzfvr1MfdnZ2SE8PBwTJ07EgQMHcOnSJQwdOhRmZmZc1UaVBpMUeq4RI0YgNDQUffr0QbNmzXD37l2taoYuvvvuO7i6uqJ169bo0aMH3nvvPdjZ2cHKysqocQDAzJkzcfPmTdSqVQvVq1fX+3yisho4cCAePXqEpk2bIiIiAmPHjsXw4cMBAHFxcRg4cCDGjx8PPz8/dO/eXasKUhbz5s1DUFAQ3n77bQQHB6Nly5YICAjQ63dHZEp8CzKZxF9//YUaNWpg7969eOONN0wdDtELIS8vDy+99BLmzp2LoUOHmjocoufinBQyiv379yM3Nxf16tVDWloaJk2aBG9vb46NE1WgpKQk/P7772jatCmys7Mxc+ZMAEBISIiJIyPSDZMUMorCwkJ89NFHuH79Ouzs7NCiRQusXr1asu/3IZKLOXPmIDk5GZaWlggMDMThw4dRrVo1U4dFpBMO9xAREZEkceIsERERSRKTFCIiIpIkJilEREQkSUxSiIiISJKYpBAREZEkMUkhIgDAoEGD0L17d83ntm3bYty4cUaP4+DBgxAEAVlZWUbvm4ikhUkKkcQNGjQIgiBAEARYWlrC19cXM2fORFFRUYX2u3nzZnzyySc6HcvEgogqAh/mRlQJvPnmm4iLi4NKpcIvv/yCiIgIWFhYIDo6Wuu4goICWFpaGqRPJycng1yHiKisWEkhqgQUCgXc3Nzg5eWFkSNHIjg4GNu2bdMM0Xz22Wfw8PCAn58fAODPP/9E79694eDgACcnJ4SEhODmzZua6xUXFyMqKgoODg5wdnbGpEmT8PRzHZ8e7lGpVJg8eTJq1KgBhUIBX19frFixAjdv3kS7du0AAI6OjhAEAYMGDQIAqNVqxMTEwMfHB9bW1njttdewceNGrX5++eUXvPLKK7C2tka7du204iSiFxuTFKJKyNraGgUFBQCAffv2ITk5GXv27MHPP/+MwsJCdOrUCXZ2djh8+DCOHj0KW1tbvPnmm5pz5s6di/j4eHz77bc4cuQI7t27hy1btvxrnwMHDsTatWuxcOFC/Pbbb/jqq69ga2uLGjVqYNOmTQCA5ORkpKWlYcGCBQCAmJgYfPfdd1i2bBkuXbqEyMhIvPvuuzh06BCAx8lUaGgounbtirNnz2LYsGH48MMPK+prI6LKRiQiSQsPDxdDQkJEURRFtVot7tmzR1QoFOKECRPE8PBw0dXVVVSpVJrjV61aJfr5+YlqtVrTplKpRGtra3HXrl2iKIqiu7u7OHv2bM3+wsJC8eWXX9b0I4qi2KZNG3Hs2LGiKIpicnKyCEDcs2dPqTEeOHBABCDev39f05afny9WrVpVPHbsmNaxQ4cOFfv16yeKoihGR0eLderU0do/efLkEtciohcT56QQVQI///wzbG1tUVhYCLVajXfeeQfTp09HREQE6tWrpzUP5dy5c7h69Srs7Oy0rpGfn49r164hOzsbaWlpaNasmWZflSpV0Lhx4xJDPk+cPXsW5ubmaNOmjc4xX716FQ8fPkSHDh202gsKCtCwYUMAwG+//aYVBwAEBQXp3AcRyRuTFKJKoF27dli6dCksLS3h4eGBKlX+/6drY2OjdWxubi4CAwOxevXqEtepXr16mfq3trbW+5zc3FwAwPbt2/HSSy9p7VMoFGWKg4heLExSiCoBGxsb+Pr66nRso0aN8MMPP8DFxQX29valHuPu7o4TJ06gdevWAICioiKcPn0ajRo1KvX4evXqQa1W49ChQwgODi6x/0klp7i4WNNWp04dKBQKpKamPrMCExAQgG3btmm1HT9+/Pk3SUQvBE6cJZKZ/v37o1q1aggJCcHhw4dx48YNHDx4EGPGjMFff/0FABg7diw+//xzbN26Fb///js++OCDf33Gibe3N8LDwzFkyBBs3bpVc83169cDALy8vCAIAn7++WfcuXMHubm5sLOzw4QJExAZGYmVK1fi2rVrOHPmDBYtWoSVK1cCAN5//32kpKRg4sSJSE5Oxpo1axAfH1/RXxERVRJMUohkpmrVqkhISICnpydCQ0MREBCAoUOHIj8/X1NZGT9+PAYMGIDw8HAEBQXBzs4OPXr0+NfrLl26FD179sQHH3wAf39/vPfee8jLywMAvPTSS5gxYwY+/PBDuLq6YtSoUQCATz75BFOmTEFMTAwCAgLw5ptvYvv27fDx8QEAeHp6YtOmTdi6dStee+01LFu2DLNmzarAb4eIKhNBfNZMOSIiIiITYiWFiIiIJIlJChEREUkSkxQiIiKSJCYpREREJElMUoiIiEiSmKQQERGRJDFJISIiIklikkJERESSxCSFiIiIJIlJChEREUkSkxQiIiKSJCYpREREJEn/B8tO6WkwBtniAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 12 :- Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,Recall, and F1-Score.\n",
        "# Ans :-\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HkfqrwtL5WW",
        "outputId": "a150c7b3-cd11-4f2f-ed76-35be095e7b37"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.9561\n",
            "Precision: 0.9459\n",
            "Recall:    0.9859\n",
            "F1 Score:  0.9655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 13 :- Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "# Ans :-\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Generate an imbalanced binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5,\n",
        "                           n_redundant=2, n_clusters_per_class=1,\n",
        "                           weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with class_weight='balanced'\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucjJ-YLhMUHc",
        "outputId": "f2ccb3d1-e22f-4f34-e7eb-7bc371d7d1ff"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[175   2]\n",
            " [  0  23]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99       177\n",
            "           1       0.92      1.00      0.96        23\n",
            "\n",
            "    accuracy                           0.99       200\n",
            "   macro avg       0.96      0.99      0.98       200\n",
            "weighted avg       0.99      0.99      0.99       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 14 :- Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "# Ans :-\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load Titanic dataset (from CSV)\n",
        "df = pd.read_csv('titanic.csv')  # Replace with your actual file path\n",
        "\n",
        "# Basic preprocessing\n",
        "# Drop columns that won't help the model\n",
        "df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
        "\n",
        "# Handle missing values\n",
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "df['Sex'] = le.fit_transform(df['Sex'])         # male=1, female=0\n",
        "df['Embarked'] = le.fit_transform(df['Embarked'])  # C=0, Q=1, S=2\n",
        "\n",
        "# Define features and target\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "3qDev32IMsJr",
        "outputId": "aa6341d7-d510-475c-da6f-b7b93e9682fe"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'titanic.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-14-2479152969.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load Titanic dataset (from CSV)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'titanic.csv'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace with your actual file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Basic preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanic.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 15 :-Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "# Ans :-\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression without scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=10000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Apply Standardization\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Logistic Regression with scaling\n",
        "model_scaled = LogisticRegression(max_iter=10000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with scaling:    {accuracy_scaled:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXb4jK1QNHD2",
        "outputId": "caeb7a46-a887-432f-9c92-bb79122699ec"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.9561\n",
            "Accuracy with scaling:    0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 16 :- Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "# Ans :-\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "DuZrGLpWNf_2",
        "outputId": "936864a8-1fa0-4fb6-a36b-ed7906742605"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9977\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 17 :- Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "# Ans :-\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with custom C (regularization strength)\n",
        "model = LogisticRegression(C=0.5, max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Logistic Regression Accuracy with C=0.5: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UPEr_bkOD9c",
        "outputId": "7a1ceaac-f335-488e-dd43-cbc8be82281a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy with C=0.5: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 18 :- Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "# Ans :-\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Extract feature importance from coefficients\n",
        "coefficients = model.coef_[0]\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Coefficient': coefficients,\n",
        "    'Importance (abs)': np.abs(coefficients)\n",
        "})\n",
        "\n",
        "# Sort by absolute importance\n",
        "feature_importance = feature_importance.sort_values(by='Importance (abs)', ascending=False)\n",
        "\n",
        "# Display top features\n",
        "print(\"Top Important Features:\")\n",
        "print(feature_importance.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM_ymcmVOc8D",
        "outputId": "1be12494-8ab7-47d8-f4a7-5e6532d2dc08"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Important Features:\n",
            "                 Feature  Coefficient  Importance (abs)\n",
            "26       worst concavity    -1.428595          1.428595\n",
            "11         texture error     1.370567          1.370567\n",
            "0            mean radius     1.027437          1.027437\n",
            "25     worst compactness    -0.772709          0.772709\n",
            "28        worst symmetry    -0.746894          0.746894\n",
            "6         mean concavity    -0.532558          0.532558\n",
            "27  worst concave points    -0.510929          0.510929\n",
            "21         worst texture    -0.508877          0.508877\n",
            "2         mean perimeter    -0.362135          0.362135\n",
            "24      worst smoothness    -0.307731          0.307731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 19 :- Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score.\n",
        "# Ans :-\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate using Cohen's Kappa Score\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mclFa9YeQ8hj",
        "outputId": "5c905c53-4d1a-4e79-ca55-ad2267f4f8b2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.9053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 20 :- Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classificatio:\n",
        "# Ans :-\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "\n",
        "# Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision and recall\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Plot the Precision-Recall Curve\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "30H_rnlBRX62",
        "outputId": "e5b15e5d-25d3-4f77-c2f5-74f37bbba16b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 21 :-  Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "# Ans :-\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "results = {}\n",
        "\n",
        "# Train and evaluate models\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=10000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results[solver] = accuracy\n",
        "\n",
        "# Print results\n",
        "print(\"Solver Comparison (Accuracy):\")\n",
        "for solver, acc in results.items():\n",
        "    print(f\"{solver}: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UiYu-XQRpKs",
        "outputId": "39d8f7be-07a5-4631-b2c8-4e99632a14d4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solver Comparison (Accuracy):\n",
            "liblinear: 0.9561\n",
            "saga: 0.9737\n",
            "lbfgs: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 22 :- Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "# Ans :-\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate using Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGigwb80R_4C",
        "outputId": "c46a9752-9b4d-456d-ba1b-4f54493b2acd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.9068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 23 :- Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "# Ans :-\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train on raw data\n",
        "model_raw = LogisticRegression(max_iter=10000)\n",
        "model_raw.fit(X_train_raw, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test_raw)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
        "X_test_scaled = scaler.transform(X_test_raw)\n",
        "\n",
        "# Train on standardized data\n",
        "model_scaled = LogisticRegression(max_iter=10000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Compare results\n",
        "print(f\"Accuracy on raw data:         {accuracy_raw:.4f}\")\n",
        "print(f\"Accuracy on standardized data: {accuracy_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCAPKv6ZSQr8",
        "outputId": "c0b4cfd2-f7af-48f3-e5d0-3d8eafe77127"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data:         0.9561\n",
            "Accuracy on standardized data: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 24 :- Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "# Ans :-\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with cross-validation to find optimal C\n",
        "model = LogisticRegressionCV(\n",
        "    Cs=10,                  # Try 10 values for C\n",
        "    cv=5,                   # 5-fold cross-validation\n",
        "    penalty='l2',           # L2 regularization\n",
        "    solver='lbfgs',         # Solver that supports L2\n",
        "    max_iter=10000,\n",
        "    scoring='accuracy',\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Output results\n",
        "print(f\"Optimal C value: {model.C_[0]:.4f}\")\n",
        "print(f\"Test Set Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNhie21LSkMA",
        "outputId": "d59f66db-7e0f-4153-d4cd-481f16f4b5bd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal C value: 166.8101\n",
            "Test Set Accuracy: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. 25 :- Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "# Ans :-\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(model, 'logistic_model.joblib')\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load('logistic_model.joblib')\n",
        "\n",
        "# Make predictions with the loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy of loaded model: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtFSy5hBS2jr",
        "outputId": "aba5d155-8f44-4d6f-e04a-950e69218008"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of loaded model: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JMYkRu0_TOra"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}